<!DOCTYPE html>
<html lang="en">

<head>
    <title>VITA</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


    <link href="https://fonts.googleapis.com/css?family=B612+Mono|Cabin:400,700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="fonts/icomoon/style.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">
    <link href="css/jquery.mb.YTPlayer.min.css" media="all" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="css/style.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->


</head>

<body data-spy="scroll" data-target=".site-navbar-target" data-offset="300">

<div class="site-wrap">

    <div class="site-mobile-menu site-navbar-target">
        <div class="site-mobile-menu-header">
            <div class="site-mobile-menu-close mt-3">
                <span class="icon-close2 js-menu-toggle"></span>
            </div>
        </div>
        <div class="site-mobile-menu-body"></div>
    </div>

    <div class="header-top">
        <div class="container" style="padding:20px">
            <div class="row align-items-center">
                <!-- <div class="col-12 col-lg-6 d-flex"> -->
                    <img src="./logo.png" width="15%">
                    <a class="ml-auto site-logo">
                       
                             &nbsp&nbsp<b style="color: rgb(71, 71, 71)">V</b>isual <b style="color: rgb(71, 71, 71)">I</b>nformatics Group @ University of <b style="color: rgb(71, 71, 71)">T</b>exas at <b style="color: rgb(71, 71, 71)">A</b>ustin
                    </a>
                    <a href="#"
                       class="ml-auto d-inline-block d-lg-none site-menu-toggle js-menu-toggle text-black"><span
                            class="icon-menu h3"></span></a>

                <!-- </div> -->
                <!-- <div class="col-12 col-lg-6 ml-auto d-flex">
                    <div class="ml-md-auto top-social d-none d-lg-inline-block">
                        <a href="#" class="d-inline-block p-3"> </a>
                        <a href="#" class="d-inline-block p-3"> </a>
                        <a href="#" class="d-inline-block p-3"> </a>
                    </div>

                </div> -->
                <!--          <div class="col-6 d-block d-lg-none text-right">-->

            </div>
        </div>
    </div>

    
    <div class="site-navbar py-2 js-sticky-header site-navbar-target d-none pl-0 d-lg-block" role="banner">

        <div class="container" style="padding-right=10%">
            <div class="d-flex align-items-right">
                <!-- <div class="mr-auto">
                    <a href="index.html">
                       <img src="./logo.png" width="10%"/>
                             &nbsp&nbsp<b>V</b>isual <b>I</b>nformatics Group @ University of <b>T</b>exas at <b>A</b>ustin
                    </a>
                </div> -->
                <div class="ml-auto">
                    <nav class="site-navigation position-relative text-right" role="navigation">
                        <ul class="site-menu main-menu js-clone-nav mr-auto d-none pl-0 d-lg-block">
                            <li class="active">
                                <a href="index.html" class="nav-link text-right">Home</a>
                            </li>
                            <li>
                                <a href="publication.html"" class="nav-link text-left">Publication</a>
                            </li>
                            <li>
                                <a href="group.html" class="nav-link text-left">Group</a>
                            </li>
                            <li>
                                <a href="resource.html" class="nav-link text-left">Resource</a>
                            </li>
                            <li>
                                <a href="prospective_students.html" class="nav-link text-left">Opening</a>
                            </li>
                            <!-- <li class="nav-item dropdown">
                                              <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                   Challenge
                                 </a>
                                          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                           <a class="dropdown-item" href="challenge1.html">Tiny Object Detection Challenge</a>
                                           <a class="dropdown-item" href="challenge2.html">Image Restoration for UDC Challenge</a>
                                         </div>
                            </li>
                            <li>
                                <a href="callforpapers.html" class="nav-link text-left">Call for Papers</a>
                            </li>

                            <li>
                                <a href="speakers.html" class="nav-link text-left">Invited Speakers</a>
                            </li>

                            <li class="nav-item dropdown">
                                <a class="nav-link dropdown-toggle" href="challenge.html" id="navbarDropdown"
                                   role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                    Previous
                                </a>
                                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                    <a class="dropdown-item" href="https://yuqian2.wixsite.com/forlq">RLQ'19</a>
                                </div>
                            </li> -->
                        </ul>
                    </nav>

                </div>

            </div>
        </div>

    </div>

</div>



<div class="site-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-12">
                <p>Our group actively publishes in the fields of machine learning, computer vision, and interdisciplinary data science. Below are a list of recent and selected papers. A mark * denotes the author to be a VITA student or Dr. Wang's mentee. An up-to-date full paper list can be found <a href="https://utexas.box.com/s/s6cneli1uegvoqp2oebvq3891jaxj7gu">here</a>.</p>
                <div class="section-title" style="margin-bottom: 30px">
                    <h2>Journal Paper</h2>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <ul>
                            <li>T. Chen*, Z. Zhang*, J. Wu, R. Huang, S. Liu, S. Chang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Can You Win Everything with A Lottery Ticket?”</b><br> Transactions on Machine Learning Research (TMLR), 2022. <a href="https://openreview.net/forum?id=JL6MU9XFzW">[Paper]</a> <a href="https://github.com/VITA-Group/LTH-Pass">[Code]</a></li>
                            <li>Y. Han*, G. Holste*, Y. Ding, A. Tewfik, Y. Peng, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays”</b><br> IEEE Transactions on Medical Imaging (TMI), 2022. <a href="https://ieeexplore.ieee.org/document/9930800">[Paper]</a> <a href="https://github.com/VITA-Group/CheXT">[Code]</a></li>
                            <li>T. Chen*, Y. Cheng, Z. Gan, J. Wang, L. Wang, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Adversarial Feature Augmentation and Normalization for Visual Recognition”</b><br> Transactions on Machine Learning Research (TMLR), 2022. <a href="https://openreview.net/forum?id=2VEUIq9Yff">[Paper]</a> <a href="https://github.com/VITA-Group/CV_A-FAN">[Code]</a></li>
                            <li>S. Mohseni, H. Wang*, Z. Yu, C. Xiao, Z. Wang, J. Yadawa<br> <b style="color:rgb(71, 71, 71)">“Taxonomy of Machine Learning Safety: A Survey and Primer”</b><br> ACM Computing Surveys (CSUR), 2022. <a href="https://dl.acm.org/doi/10.1145/3551385">[Paper]</a> </li>
                             <li>T. Chen*, S. Liu, S. Chang, L. Amini, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning”</b><br> Transactions on Machine Learning Research (TMLR), 2022. <a href="https://openreview.net/forum?id=oLvlPJheCD">[Paper]</a> <a href="https://github.com/VITA-Group/CIL-QUD">[Code]</a></li>
                            <li> (α-β) T. Chen*, X. Chen*, W. Chen*, H. Heaton, J. Liu, and Z. Wang, W. Yin<br> <b style="color:rgb(71, 71, 71)">“Learning to Optimize: A Primer and A Benchmark”</b><br> Journal of Machine Learning Research (JMLR), 2022. <a href="https://jmlr.org/papers/v23/21-0308.html">[Paper]</a> <a href="https://github.com/VITA-Group/Open-L2O">[Code]</a></li>
                            <li>T. Chen*, K. Zhou, K. Duan, W. Zheng*, P. Wang*, X. Hu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)"> “Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study”</b><br> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022. <a href="https://arxiv.org/abs/2108.10521">[Paper]</a> <a href="https://github.com/VITA-Group/Deep_GCN_Benchmarking">[Code]</a></li>
                            <li> Z. Chen, Y. Jiang*, D. Liu, Z. Wang<br> <b style="color:rgb(71, 71, 71)">“CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise”</b><br> IEEE Transactions on Image Processing (TIP), 2022. <a href="https://arxiv.org/abs/2108.00478">[Paper]</a> <a href="https://github.com/VITA-Group/CERL">[Code]</a></li>
                            <li>C. Li, W. Chen*, Y. Gu, T. Chen*, Y. Fu, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference”</b><br>ACM Transactions on Design Automation of Electronic Systems (TODAES), 2022. <a href="https://arxiv.org/abs/2107.07706">[Paper]</a> <a href="">[Code]</a></li>
                            <li>X. Chen*, Y. Zhao, Y. Wang, P. Xu, H. You, C. Li, Y. Fu, Y. Lin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“SmartDeal: Re-Modeling Deep Network Weights for Efficient Inference and Training”</b><br>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2021. <a href="https://arxiv.org/pdf/2101.01163.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/SmartDeal">[Code]</a></li>
                            <li>T. Hu*, F. Gama, T. Chen*, W. Zheng*, Z. Wang, A. Ribeiro, and B. Sadler<br> <b style="color:rgb(71, 71, 71)">“Scalable Perception-Action-Communication Loops with Convolutional and Graph Neural Networks”</b><br>IEEE Transactions on Signal and Information Processing over Networks (TSIPN), 2021. <a href="https://arxiv.org/abs/2106.13358">[Paper]</a> <a href="https://github.com/VITA-Group/VGAI">[Code]</a></li>
                            <li>H. Wang*, T. Chen*, Z. Wang, and K. Ma<br> <b style="color:rgb(71, 71, 71)">“Troubleshooting Image Segmentation Models with Human-In-The-Loop”</b><br> Springer Machine Learning, 2021. <a href="https://trebuchet.public.springernature.app/get_content/6f6996c6-dd27-4266-af36-af70aa6d70a0">[Paper]</a> <a href="https://github.com/VITA-Group/Troubleshooting_Image_Segmentation">[Code]</a></li>
                            <li>S. Yang*, Z. Wang, J. Jiu, and Z. Guo<br> <b style="color:rgb(71, 71, 71)">“Controllable Sketch-to-Image Translation for Robust Face Synthesis”</b><br> IEEE Transactions on Image Processing (TIP), 2021. <a href="https://ieeexplore.ieee.org/document/9583954">[Paper]</a> <a href="https://github.com/VITA-Group/DeepPS">[Code]</a></li>
                            <li>Z. Wu*, Z. Wang, Y. Yuan*, J. Zhang, Z. Wang, and H. Jin<br> <b style="color:rgb(71, 71, 71)">“Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot Study”</b><br> ACM Transactions on Multimedia Computing Communications and Applications (ToMM), 2021. <a href="https://arxiv.org/abs/2107.12202">[Paper]</a> <a href="https://github.com/VITA-Group/BlackBoxGANCollapse">[Code]</a></li>
                             <li>J. Yan, Y. Zhong, Y. Fang, Z. Wang, and K. Ma<br> <b style="color:rgb(71, 71, 71)">“Exposing Semantic Segmentation Failures via Maximum Discrepancy Competition”</b><br> International Journal of Computer Vision (IJCV), 2021. <a href="https://arxiv.org/abs/2103.00259">[Paper]</a> <a href="https://github.com/QTJiebin/MAD_Segmentation">[Code]</a></li>
                            <li>S. Yang*, Z. Wang, and J. Liu<br> <b style="color:rgb(71, 71, 71)">“Shape-Matching GAN++: Scale Controllable Dynamic Artistic Text Style Transfer”</b><br> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. <a href="https://ieeexplore.ieee.org/document/9339900">[Paper]</a></li>
                            <li>Z. Wu*, H. Wang*, Z. Wang, H. Jin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset”</b><br> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9207852">[Paper]</a> <a href="https://github.com/VITA-Group/PA-HMDB51">[Code]</a></li>
                            <li>Y. Jiang*, X. Gong*, D. Liu, Y. Cheng, C. Fang, X. Shen, J. Yang, P. Zhou, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“EnlightenGAN: Deep Light Enhancement without Paired Supervision”</b><br> IEEE Transactions on Image Processing (TIP), 2020. <a href="https://arxiv.org/abs/1906.06972">[Paper]</a> <a href="https://github.com/VITA-Group/EnlightenGAN">[Code]</a></li>
                              <li> M. Karimi, D. Wu, Z. Wang, and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“Explainable Deep Relational Networks for Predicting Compound-Protein Affinities and Contacts”</b><br> Journal of Chemical Information and Modeling (JCIM), 2020. <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.0c00866">[Paper]</a> [Code] </li>
                             <li>S. Li, W. Ren, F. Wang, I. Araujo*, E. K. Tokuda*, R. Hirata, R. Cesar, Z. Wang, and X. Cao<br> <b style="color:rgb(71, 71, 71)">“A Comprehensive Benchmark Analysis of Single Image Deraining: Current Challenges and Future Perspectives”</b><br> International Journal of Computer Vision (IJCV), 2020.  <a href="https://link.springer.com/article/10.1007/s11263-020-01416-w">[Paper]</a> </li>
                            <li>Y. Yuan*, W. Yang, W. Ren, J Liu, W. J. Scheirer, and Z. Wang, et. al.<br> <b style="color:rgb(71, 71, 71)">“Advancing Image Understanding in Poor Visibility Environments: A Collective Benchmark Study”</b><br> IEEE Transactions on Image Processing (TIP), 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9049390">[Paper]</a></li>
                            <li>M. Karimi, D. Wu, Z. Wang and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity through Unified Recurrent and Convolutional Neural Networks”</b><br> Oxford Bioinformatics, 2019. <a href="https://academic.oup.com/bioinformatics/article/35/18/3329/5320555">[Paper]</a> <a href="https://github.com/Shen-Lab/DeepAffinity">[Code]</a></li>
                            <li>R. G. VidalMata, ... Y. Yuan*, J. Wu*, Z. Wang, ... et. al. <br> <b style="color:rgb(71, 71, 71)">“Bridging the Gap Between Computational Photography and Visual Recognition”</b><br> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9097964">[Paper]</a><a href="https://github.com/VITA-Group/TAMU-PKU-UG2">[Code]</a></li>
                            <li>Y. Wang*, J. Shen*, T. Hu*, T. Nguyen, R. Baraniuk, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“Dual Dynamic Inference: Enabling More Efficient, Adaptive and Controllable Deep Inference”</b><br> IEEE Journal of Selected Topics in Signal Processing (JSTSP), 2020. <a href="https://ieeexplore.ieee.org/abstract/document/9028245">[Paper]</a></li>
                            <li>B. Li*, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Benchmarking Single Image Dehazing and Beyond”</b><br> IEEE Transactions on Image Processing (TIP), vol. 28, no. 1, pp. 492-505, 2019. <a href="https://ieeexplore.ieee.org/abstract/document/8451944">[Paper]</a> <a href="https://sites.google.com/site/boyilics/website-builder/reside">[Project Page]</a></li>
                        </ul>
                    </div>
                </div>
                <div class="section-title" style="margin-bottom: 30px">
                    <h2>Conference Paper</h2>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <ul>
                            <li>H. Heaton, X. Chen*, Z. Wang, and W. Yin<br> <b style="color:rgb(71, 71, 71)">"Safeguarded Learned Convex Optimization”</b><br>AAAI Conference on Artificial Intelligence (AAAI), 2023. <a href="https://arxiv.org/abs/2003.01880">[Paper]</a> [Code] </li>
                            <li>J. Hong, H. Wang*, Z. Wang, and J. Zhou<br> <b style="color:rgb(71, 71, 71)">"Federated Robustness Propagation: Sharing Adversarial Robustness in Heterogeneous Federated Learning”</b><br>AAAI Conference on Artificial Intelligence (AAAI), 2023. <a href="https://arxiv.org/abs/2106.10196">[Paper]</a> [Code] </li>
                            <li>Z. Kong, H. Ma, G. Yuan, M. Sun, Y. Xie, P. Dong, X. Meng, X. Shen, H. Tang, M. Qin, T. Chen*, X. Ma, X. Xie, Z. Wang, and Y. Wang<br> <b style="color:rgb(71, 71, 71)">"Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training”</b><br>AAAI Conference on Artificial Intelligence (AAAI), 2023. <a href="https://arxiv.org/abs/2211.10801">[Paper]</a> <a href="https://github.com/ZLKong/Tri-Level-ViT">[Code] </a> </li>
                            <li>T. Huang, T. Chen*, M. Fang, V. Menkovski, J. Zhao, L. Yin, Y. Pei, D. Mocanu, Z. Wang, M. Pechenizkiy, and S. Liu*<br> <b style="color:rgb(71, 71, 71)">"You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained Graph Tickets”</b><br>Learning on Graphs Conference (LoG), 2022. (Oral) <a href="https://openreview.net/forum?id=dF6aEW3_62O">[Paper]</a> [Code] </li>
                            <li>Y. Han*, E. Huang, W. Zheng*, N. Rao, Z. Wang, and K. Subbian<br> <b style="color:rgb(71, 71, 71)">“Search Behavior Prediction: A Hypergraph Perspective”</b><br>ACM International Conference on Web Search and Data Mining (WSDM), 2023. <a href="https://arxiv.org/abs/2211.13328">[Paper]</a> <a href="https://github.com/amazon-science/dual-channel-hypergraph-neural-network">[Code] </a> </li>
                            <li>W. Chen*, W. Huang, X. Gong*, B. Hanin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2205.05662">[Paper]</a> <a href="https://github.com/VITA-Group/architecture_convergence">[Code] </a> </li>
                            <li>D. Xu*, P. Wang*, Y. Jiang*, Z. Fan*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Signal Processing for Implicit Neural Representations”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.08772">[Paper]</a> <a href="https://vita-group.github.io/INSP/">[Code] </a> </li>
                            <li>H. Liang*, Z. Fan*, R. Sarkar, Z. Jiang*, T. Chen*, K. Zou, Y. Cheng, C. Hao, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“M<sup>3</sup>ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.14793">[Paper]</a> <a href="https://github.com/VITA-Group/M3ViT">[Code] </a> </li>
                            <li>Z. Jiang*, X. Chen*, X. Huang, X. Du, D. Zhou, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropogation”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://openreview.net/forum?id=mTXQIpXPDbh">[Paper]</a>  <a href="https://github.com/VITA-Group/BackRazor_Neurips22">[Code] </a> </li>
                            <li>R. Cai*, Z. Zhang*, T. Chen*, X. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://openreview.net/forum?id=TItRK4VP9X2">[Paper]</a>  <a href="https://github.com/VITA-Group/Random-Shuffling-BackdoorDetect">[Code] </a> </li>
                            <li>S. Sharan*, W. Zheng*, K. Hsu, J. Xiong, A. Chen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Symbolic Distillation for Learned TCP Congestion Control”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.16987">[Paper]</a> <a href="https://github.com/VITA-Group/SymbolicPCC">[Code] </a> </li>
                            <li>A. Jaiswal*, P. Wang*, T. Chen*, J. Rousseau, Y. Ding, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Old can be Gold: Better Gradient Flow can make Vanilla-GCNs Great Again”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.08122">[Paper]</a> <a href="https://github.com/VITA-Group/GradientGCN">[Code]</a></li>
                            <li>H. Wang*, J. Hong, A. Zhang, J. Zhou, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.06428">[Paper]</a> <a href="https://github.com/VITA-Group/Trap-and-Replace-Backdoor-Defense">[Code]</a> </li>
                            <li>J. Wu*, Y. Liang, F. Han, H. Akbari, Z. Wang, and C. Yu<br> <b style="color:rgb(71, 71, 71)">“Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://openreview.net/forum?id=7WuCttgNQ79">[Paper]</a> [Code] </li>
                            <li>M. Varma*, X. Chen*, Z. Zhang*, T. Chen*, S. Venugopalan, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Sparse Winning Tickets are Data-Efficient Image Recognizers”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://openreview.net/forum?id=wfKbtSjHA6F">[Paper]</a> <a href="https://github.com/VITA-Group/DataEfficientLTH">[Code]</a> </li>
                            <li>T. Wei, Y. You*, T. Chen*, Y. Shen, J. He, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2022. <a href="https://arxiv.org/abs/2210.03801">[Paper]</a> <a href="https://github.com/weitianxin/HyperGCL">[Code]</a> </li>
                            <li>K. Duan, Z. Liu, P. Wang*, W. Zheng*, K. Zhou, T. Chen*, X. Hu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking”</b><br>Advances in Neural Information Processing Systems, Track on Datasets and Benchmarks (NeurIPS Benchmark), 2022. <a href="https://openreview.net/forum?id=2QrFr_U782Z">[Paper]</a> <a href="https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking">[Code] </a>  </li>
                            <li>D. Xu*, Y. Jiang*, P. Wang*, Z. Fan*, H. Shi, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“SinNeRF: Training Neural Radiance Field on Complex Scenes from a Single Image”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://arxiv.org/abs/2204.00928">[Paper]</a> <a href="https://vita-group.github.io/SinNeRF/">[Code] </a> </li>
                            <li>Z. Fan*, Y. Jiang*, P. Wang*, X. Gong*, D. Xu*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Unified Implicit Neural Stylization”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://arxiv.org/abs/2204.01943">[Paper]</a> <a href="https://zhiwenfan.github.io/INS/">[Code] </a> </li>
                            <li>X. Chen*, T. Chen*, Y. Cheng, W. Chen, A. Awadallah, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Scalable Learning to Optimize: A Learned Optimizer Can Train Big Models”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830376.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/Scalable-L2O">[Code] </a> </li>
                            <li>H. Liang*, H. Fan*, Z. Fan*, Y. Wang*, T. Chen*, Y. Cheng, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Point Cloud Domain Adaptation via Masked Local 3D Structure Prediction”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136630159.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/MLSP">[Code] </a> </li>
                            <li> Z. Jiang*, T. Chen*, X. Chen*, Y. Cheng, L. Zhou, L. Yuan, A. Awadallah, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“DnA: Improving Few-shot Transfer Learning with Low-Rank Decomposition and Alignment”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800229.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/DnA">[Code] </a> </li>
                            <li> Y. Jiang*, B. Wronski, B. Mildenhall, J. Barron, Z. Wang, and T. Xue<br> <b style="color:rgb(71, 71, 71)">“Fast and High Quality Image Denoising via Malleable Convolution”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://arxiv.org/abs/2201.00392">[Paper]</a> <a href="https://yifanjiang.net/MalleConv.html">[Code] </a> </li>
                            <li> W. Chen*, X. Du, F. Yang, L. Beyer, X. Zhai, T. Lin, H. Chen, J. Li, X. Song, Z. Wang, and D. Zhou<br> <b style="color:rgb(71, 71, 71)">“A Simple Single-Scale Vision Transformer for Object Detection and Instance Segmentation”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://arxiv.org/abs/2112.09747">[Paper]</a> [Code] </li>
                            <li> Z. Mao, A. Jaiswal*, Z. Wang, and S. Chan<br> <b style="color:rgb(71, 71, 71)">“Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model”</b><br>European Conference on Computer Vision (ECCV), 2022. <a href="https://arxiv.org/abs/2207.10040">[Paper]</a> <a href="https://github.com/VITA-Group/TurbNet">[Code] </a> </li>
                            <li>D. Xu*, H. Poghosyan, S. Navasardyan, Y. Jiang*, H. Shi, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“ReCoRo: Region-Controllable Robust Light Enhancement by User-Specified Imprecise Masks”</b><br>ACM International Conference on Multimedia (ACM MM), 2022. <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547813">[Paper]</a> <a href="https://github.com/Picsart-AI-Research/ReCoRo-Controllable-Low-Light-Image-Enhancement">[Code]</a> </li>
                            <li>Z. Wu*, Z. Ren, Y. Wu, Z. Wang, and G. Hua<br> <b style="color:rgb(71, 71, 71)">“TxVAD: Improved Video Action Detection by Transformers”</b><br>ACM International Conference on Multimedia (ACM MM), 2022. <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547992">[Paper]</a> [Code] </li>
                            <li>Z. Wan, D. Xu*, Z. Wang, J. Wang, and J. Luo<br> <b style="color:rgb(71, 71, 71)">“Cloud2Sketch: Augmenting Clouds with Imaginary Sketches”</b><br>ACM International Conference on Multimedia (ACM MM), 2022. <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547810">[Paper]</a> <a href="https://wanzy.me/research/cloud2sketch">[Code] </a> </li>
                            <li>H. Wang*, A. Zhang, Y. Zhu, S. Zheng, M. Li, A. Smola, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Partial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition”</b><br>International Conference on Machine Learning (ICML), 2022. (Long Talk) <a href="https://proceedings.mlr.press/v162/wang22aq/wang22aq.pdf">[Paper]</a> <a href="https://github.com/amazon-research/long-tailed-ood-detection">[Code]</a> </li>
                            <li>H. Wang*, A. Zhang, S. Zheng, X. Shi, M. Li, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Removing Batch Normalization Boosts Adversarial Training”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/wang22ap/wang22ap.pdf">[Paper]</a> <a href="https://github.com/amazon-research/normalizer-free-robust-training">[Code]</a> </li>
                            <li>P. Wang*, W. Zheng*, T. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Neural Implicit Dictionary Learning via Mixture-of-Expert Training”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/wang22d/wang22d.pdf">[Paper]</a>  <a href="https://github.com/VITA-Group/Neural-Implicit-Dict">[Code]</a> </li>
                            <li>A. Jaiswal*, H. Ma, T. Chen*, Y. Ding, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Training Your Sparse Neural Network Better with Any Mask”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/jaiswal22a/jaiswal22a.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/ToST">[Code]</a> </li>
                            <li>T. Chen*, H. Zhang, Z. Zhang*, S. Chang, S. Liu, P. Chen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Linearity Grafting: How Neuron Pruning Helps Certifiable Robustness”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/chen22af/chen22af.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/Linearity-Grafting">[Code]</a> </li>
                            <li>T. Chen*, X. Chen*, X. Ma, Y. Wang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/chen22a/chen22a.pdf">[Paper]</a>  <a href="https://github.com/VITA-Group/Structure-LTH">[Code]</a></li>
                            <li>T. Chen*, Z. Zhang*, S. Liu, Y. Zhang, S. Chang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Data-Efficient Double-Win Lottery Tickets from Robust Pre-training”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/chen22ae/chen22ae.pdf">[Paper]</a>  <a href="https://github.com/VITA-Group/Double-Win-LTH">[Code]</a> </li>
                            <li>W. Redman, T Chen*, Z. Wang, and A. Dogra,<br> <b style="color:rgb(71, 71, 71)">“Universality of Winning Tickets: A Renormalization Group Perspective”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/redman22a/redman22a.pdf">[Paper]</a> [Code]</li>
                            <li>R. Ardywibowo, Z. Huo, Z. Wang, B. Mortazavi, S. Huang, and X. Qian<br> <b style="color:rgb(71, 71, 71)">“VariGrow: Variational Architecture Growing for Task-Agnostic Continual Learning based on Bayesian Novelty”</b><br>International Conference on Machine Learning (ICML), 2022. <a href="https://proceedings.mlr.press/v162/ardywibowo22a/ardywibowo22a.pdf">[Paper]</a>  [Code] </li>
                            <li>D. Hoang*, K. Zhou, T. Chen*, X. Hu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“AutoCoG: A Unified Data-Model Co-Search Framework for Graph Neural Networks”</b><br>International Conference on Automated Machine Learning (AutoML-Conf), 2022. <a href="https://openreview.net/forum?id=r0zIWWar8gq">[Paper]</a> <a href="https://github.com/VITA-Group/AutoCoG"> [Code]</a></li>
                            <li>J. Hong, Z. Wang, and J. Zhou<br> <b style="color:rgb(71, 71, 71)">“Dynamic Privacy Budget Allocation Improves Data Efficiency of Differentially Private Gradient Descent”</b><br>ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022. <a href="https://arxiv.org/abs/2101.07413">[Paper]</a> [Code]</li>
                            <li>T. Chen*, Z. Zhang*, Y. Cheng, A. Awadallah, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_The_Principle_of_Diversity_Training_Stronger_Vision_Transformers_Calls_for_CVPR_2022_paper.pdf">[Paper]</a>  <a href="https://github.com/VITA-Group/Diverse-ViT">[Code]</a></li>
                            <li>T. Chen*, P. Wang*, Z. Fan*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Aug-NeRF: Training Stronger Neural Radiance Fields with Triple-Level Physically-Grounded Augmentations”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Aug-NeRF_Training_Stronger_Neural_Radiance_Fields_With_Triple-Level_Physically-Grounded_Augmentations_CVPR_2022_paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/Aug-NeRF">[Code]</a></li>
                            <li>Z. Fan*, T. Chen*, P. Wang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawing”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. (Oral) <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/CADTransformer">[Code]</a></li>
                            <li>T. Chen*, Z. Zhang*, Y. Zhang, S. Chang, S. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Quarantine_Sparsity_Can_Uncover_the_Trojan_Attack_Trigger_for_Free_CVPR_2022_paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/Backdoor-LTH">[Code]</a></li>
                            <li>X. Sun, A. Hassani, Z. Wang, G. Huang, and H. Shi<br> <b style="color:rgb(71, 71, 71)">“DiSparse: Disentangled Sparsification for Multitask Model Compression”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_DiSparse_Disentangled_Sparsification_for_Multitask_Model_Compression_CVPR_2022_paper.pdf">[Paper]</a>  <a href="https://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression">[Code]</a></li>
                            <li>Z. Chen, Y. Chen, J. Liu, X. Xu, V. Goel, Z. Wang, H. Shi, and X. Wang<br> <b style="color:rgb(71, 71, 71)">“VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VideoINR_Learning_Video_Implicit_Neural_Representation_for_Continuous_Space-Time_Super-Resolution_CVPR_2022_paper.pdf">[Paper]</a>  <a href="https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution">[Code]</a></li>
                            <li>H. Ma, H. Zhao, Z. Lin, A. Kale, Z. Wang, T. Yu, J. Gu, S. Choudhary, and X. Xie<br> <b style="color:rgb(71, 71, 71)">“EI-CLIP: Entity-aware Interventional Contrastive Learning for E-commerce Cross-modal Retrieval”</b><br>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_EI-CLIP_Entity-Aware_Interventional_Contrastive_Learning_for_E-Commerce_Cross-Modal_Retrieval_CVPR_2022_paper.pdf">[Paper]</a>  <a href="">[Code]</a></li>
                            <li>W. Zheng*, T. Chen*, T. Hu*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Symbolic Learning to Optimize: Towards Interpretability and Scalability”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=ef0nInZHKIC">[Paper]</a> <a href="https://github.com/VITA-Group/Symbolic-Learning-To-Optimize">[Code]</a></li>
                            <li>X. Chen*, J. Zhang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=moHCzz6D5H3">[Paper]</a> <a href="https://github.com/VITA-Group/Peek-a-Boo">[Code]</a></li>
                            <li>T. Huang*, T. Chen*, S. Liu, S. Chang, L. Amini, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Optimizer Amalgamation”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=VqzXzA9hjaX">[Paper]</a> <a href="https://github.com/VITA-Group/Optimizer_Amalgamation">[Code]</a></li>
                            <li>T. Chen*, Z. Zhang*, P. Wang, S. Balachandra*, H. Ma, Z. Wang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Sparsity Winning Twice: Better Robust Generalization from More Efficient Training”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=SYuJXrXq8tw">[Paper]</a> <a href="https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization">[Code]</a></li>
                            <li>P. Wang*, W. Zheng*, T. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=O476oWmiNNp">[Paper]</a> <a href="https://github.com/VITA-Group/ViT-Anti-Oversmoothing">[Code]</a></li>
                            <li>W. Chen*, W Huang, X. Du, X. Song, Z. Wang, and D. Zhou<br> <b style="color:rgb(71, 71, 71)">“Auto-Scaling Vision Transformers without Training”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=H94a1_Pyr-6">[Paper]</a> <a href="https://github.com/VITA-Group/AsViT">[Code]</a></li>
                            <li>S. Yu*, T. Chen*, J. Shen*, H. Yuan, J. Tian, S. Yang, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Unified Visual Transformer Compression”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=9jsZiUgkCZP">[Paper]</a> <a href="https://github.com/VITA-Group/UVC">[Code]</a></li>
                            <li>M. Lu*, X. Luo*, T. Chen*, W. Chen*, D. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining”</b><br>International Conference on Learning Representations (ICLR), 2022. (Spotlight Oral) <a href="https://openreview.net/forum?id=O1DEtITim__">[Paper]</a> <a href="https://github.com/VITA-Group/SFW-Once-for-All-Pruning">[Code]</a></li>
                            <li>W. Zheng*, E. Huang, N. Rao, S. Katariya, Z. Wang, and K. Subbian<br> <b style="color:rgb(71, 71, 71)">“Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=1ugNpm7W6E">[Paper]</a> <a href="https://github.com/amazon-research/gnn-tail-generalization">[Code]</a></li>
                            <li>S. Ding, T. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=9Nk6AJkVYB">[Paper]</a> <a href="https://github.com/VITA-Group/Audio-Lottery">[Code]</a></li>
                            <li>J. Hong, H. Wang*, Z. Wang, and J. Zhou<br> <b style="color:rgb(71, 71, 71)">“Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=_QLmakITKg">[Paper]</a> <a href="https://github.com/illidanlab/SplitMix">[Code]</a></li>
                            <li>S. Liu, T. Chen*, Z. Atashgahi, X. Chen*, G. Sokar, E. Mocanu, M. Pechenizkiy, Z. Wang, and D. Mocanu<br> <b style="color:rgb(71, 71, 71)">“Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=RLtqs6pzj1-">[Paper]</a> <a href="https://github.com/VITA-Group/FreeTickets">[Code]</a></li>
                            <li>S. Liu, T. Chen*, X. Chen*, L. Shen, D. Mocanu, Z. Wang, and M. Pechenizkiy<br> <b style="color:rgb(71, 71, 71)">“The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=VBZJ_3tz-t">[Paper]</a> <a href="https://github.com/VITA-Group/Random_Pruning">[Code]</a></li>
                            <li>Y. You*, Y. Cao, T. Chen*, Z. Wang, and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How”</b><br>International Conference on Learning Representations (ICLR), 2022. <a href="https://openreview.net/forum?id=EVVadRFRgL7">[Paper]</a> <a href="https://github.com/Shen-Lab/Bayesian-L2O">[Code]</a></li>
                            <li> R. Ardywibowo, S. Boluki, Z. Wang, B. Mortazavi, S. Huang, and X. Qian<br> <b style="color:rgb(71, 71, 71)">“VFDS: Variational Foresight Dynamic Selection in Bayesian Neural Networks for Efficient Human Activity Recognition”</b><br>International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. <a href="https://arxiv.org/abs/2204.00130">[Paper]</a> <a href="">[Code]</a></li>

                           <li> S. Bibikar, H. Vikalo, Z. Wang, and X. Chen* (X. C. as corresponding author)<br> <b style="color:rgb(71, 71, 71)">“Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better”</b><br>AAAI Conference on Artificial Intelligence (AAAI), 2022. <a href="https://arxiv.org/abs/2112.09824">[Paper]</a> <a href="https://github.com/bibikar/feddst">[Code]</a></li>

                             <li>Y. You*, T. Chen*, Z. Wang and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations”</b><br>ACM International Conference on Web Search and Data Mining (WSDM), 2022. <a href="https://arxiv.org/abs/2201.01702">[Paper]</a> <a href="https://github.com/Shen-Lab/GraphCL_Automated">[Code]</a></li>

                            <li>Y. Jiang*, S. Chang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2102.07074">[Paper]</a> <a href="https://github.com/VITA-Group/TransGAN">[Code]</a></li>

                            <li>H. Wang*, C. Xiao, J. Kossaifi, Z. Yu, A. Anandkumar, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“AugMax: Adversarial Composition of Random Augmentations for Robust Training”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2110.13771">[Paper]</a> <a href="https://github.com/VITA-Group/AugMax">[Code]</a></li>

                            <li>T. Chen*, Y. Cheng, Z. Gan, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2103.00397">[Paper]</a> <a href="https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training">[Code]</a></li>

                            <li>X. Chen*, Y. Cheng, S. Wang, Z. Gan, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“The Elastic Lottery Ticket Hypothesis”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2103.16547">[Paper]</a> <a href="https://github.com/VITA-Group/ElasticLTH">[Code]</a></li>

                            <li>T. Chen*, Y. Cheng, Z. Gan, L. Yuan, L. Zhang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Chasing Sparsity in Vision Transformers: An End-to-End Exploration”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2106.04533">[Paper]</a> <a href="https://github.com/VITA-Group/SViTE">[Code]</a></li>

                            <li>W. Zheng*, Q. Guo, H. Yang, P. Wang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2110.15926">[Paper]</a> <a href="https://github.com/VITA-Group/DePT">[Code]</a></li>

                            <li>X. Chen*, T. Chen*, Z. Zhang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“You Are Caught Stealing My Winning Lottery Ticket! Making a Lottery Ticket Claim its Ownership”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2111.00162">[Paper]</a> <a href="https://github.com/VITA-Group/NO-stealing-LTH">[Code]</a></li>

                            <li>Z. Jiang*, T. Chen*, T. Chen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Improving Contrastive Learning on Imbalanced Seed Data via Open-World Sampling”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2111.01004">[Paper]</a> <a href="https://github.com/VITA-Group/MAK">[Code]</a></li>

                            <li>X. Chen*, J. Liu, Z. Wang, W. Yin<br> <b style="color:rgb(71, 71, 71)">“Hyperparameter Tuning is All You Need for LISTA”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2110.15900">[Paper]</a> <a href="https://github.com/VITA-Group/HyperLISTA">[Code]</a></li>

                            <li>J. Wu*, X. Dai, D. Chen, Y. Chen, M. Liu, Y. Yu, Z. Wang, Z. Liu, M. Chen, and L. Yuan<br> <b style="color:rgb(71, 71, 71)">“Stronger NAS with Weaker Predictors”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2102.10490">[Paper]</a> <a href="https://github.com/VITA-Group/WeakNAS">[Code]</a></li>

                            <li>B. Pan, R. Panda, Y. Jiang*, Z. Wang, R. Feris, and A. Oliva<br> <b style="color:rgb(71, 71, 71)">“IA-RED<sup>2</sup>: Interpretability-Aware Redundancy Reduction for Vision Transformers”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2106.12620">[Paper]</a> <a href="http://people.csail.mit.edu/bpan/ia-red/">[Code]</a></li>

                            <li>S. Liu, T. Chen*, X. Chen*, Z. Atashgahi, L. Yin, H. Kou, L. Shen, M. Pechenizkiy, Z. Wang, and D. Mocanu<br> <b style="color:rgb(71, 71, 71)">“Sparse Training via Boosting Pruning Plasticity with Neuroregeneration”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2106.10404">[Paper]</a> <a href="https://github.com/VITA-Group/GraNet">[Code]</a></li>

                            <li>X. Ma, G. Yuan, X. Shen, T. Chen*, X. Chen*, X. Chen*, N. Liu, M. Qin, S. Liu, Z. Wang, and Y. Wang<br> <b style="color:rgb(71, 71, 71)">“Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?”</b><br>Advances in Neural Information Processing Systems (NeurIPS), 2021. <a href="https://arxiv.org/abs/2107.00166">[Paper]</a> <a href="https://github.com/boone891214/sanity-check-LTH">[Code]</a></li>

                            <li> Y. Jiang*, H. Zhang, J. Zhang, Y. Wang, Z. Lin, K. Sunkavalli, S. Chen, S. Amirghods, S. Kong, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“SSH: A Self-Supervised Framework for Image Harmonization”</b><br> IEEE International Conference on Computer Vision (ICCV), 2021. <a href="https://arxiv.org/pdf/2108.06805.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/SSHarmonization">[Code]</a></li>
                            <li> X. Gong*, H. Wang, M. Shou, M. Feiszli, Z. Wang, and Z. Yan<br> <b style="color:rgb(71, 71, 71)">“Searching for Two-Stream Models in Multivariate Space for Video Recognition”</b><br> IEEE International Conference on Computer Vision (ICCV), 2021. <a href="https://arxiv.org/abs/2108.12957">[Paper]</a> [Code]</li>
                             <li> Y. Guo, H. Yuan, J. Tan, Z. Wang, S. Yang, and J. Liu<br> <b style="color:rgb(71, 71, 71)">“GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization”</b><br> IEEE International Conference on Computer Vision (ICCV), 2021. <a href="https://arxiv.org/abs/2109.02220">[Paper]</a> [Code]</li>
                            <li>Y. You*, T. Chen*, Y. Shen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Graph Contrastive Learning Automated”</b><br> International Conference on Machine Learning (ICML), 2021. (Long Talk) <a href="https://arxiv.org/abs/2106.07594">[Paper]</a><a href="https://github.com/Shen-Lab/GraphCL_Automated">[Code]</a> </li>
                            <li>M. Zhu*, T. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm”</b><br>  International Conference on Machine Learning (ICML), 2021. (Long Talk) <a href="https://arxiv.org/abs/2106.06027">[Paper]</a><a href="https://github.com/VITA-Group/SparseADV_Homotopy">[Code]</a> </li>
                            <li> T. Chen*, Y. Sui, X. Chen*, A. Zhang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“A Unified Lottery Ticket Hypothesis for Graph Neural Networks”</b><br>  International Conference on Machine Learning (ICML), 2021. <a href="https://arxiv.org/abs/2102.06790">[Paper]</a><a href="https://github.com/VITA-Group/Unified-LTH-GNN">[Code]</a> </li>
                            <li> Z. Jiang*, T. Chen*, B. Mortazavi, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Self-Damaging Contrastive Learning”</b><br>  International Conference on Machine Learning (ICML), 2021. <a href="https://arxiv.org/abs/2106.02990">[Paper]</a><a href="https://github.com/VITA-Group/SDCLR">[Code]</a> </li>
                            <li> Z. Zhang*, X. Chen*, T. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Efficient Lottery Ticket Finding: Less Data is More”</b><br>  International Conference on Machine Learning (ICML), 2021. <a href="https://arxiv.org/abs/2106.03225">[Paper]</a><a href="https://github.com/VITA-Group/PrAC-LTH">[Code]</a> </li>
                            <li>X. Chen*, Y. Cheng, S. Wang, Z. Gan, Z. Wang, and J. Liu<br> <b style="color:rgb(71, 71, 71)">“EarlyBERT: Efficient BERT Training via Early-Bird Lottery Tickets”</b><br>  Annual Meeting of the Association for Computational Linguistics (ACL), 2021. <a href="https://arxiv.org/abs/2101.00063">[Paper]</a><a href="https://github.com/VITA-Group/EarlyBERT">[Code]</a> </li>
                            <li> J. Hong, Z. Zhu, S. Yu, Z. Wang, H. Dodge, and J. Zhou<br> <b style="color:rgb(71, 71, 71)">“Federated Adversarial Debiasing for Fair and Transferable Representations”</b><br>  ACM Conference on Knowledge Discovery and Data Mining (KDD), 2021. <a href="https://dl.acm.org/doi/10.1145/3447548.3467281">[Paper]</a> <a href="https://github.com/illidanlab/FADE">[Code]</a> </li>
                            <li>T. Chen*, J. Frankle, S. Chang, S. Liu, Y. Zhang, M. Carbin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <a href="https://arxiv.org/abs/2012.06908">[Paper]</a><a href="https://github.com/VITA-Group/CV_LTH_Pre-training">[Code]</a></li>
                            <li>Z. Wang, H. Wang*, T. Chen*, Z. Wang, and K. Ma<br> <b style="color:rgb(71, 71, 71)">“Troubleshooting Blind Image Quality Models in the Wild”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <a href="https://arxiv.org/abs/2105.06747">[Paper]</a> <a href="https://github.com/wangzhihua520/troubleshooting_BIQA">[Code]</a></li>
                            <li>P. Cao, Z. Wang, and K. Ma<br> <b style="color:rgb(71, 71, 71)">“Debiased Subjective Assessment of Real-World Image Enhancement”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cao_Debiased_Subjective_Assessment_of_Real-World_Image_Enhancement_CVPR_2021_paper.pdf">[Paper]</a> [Code]    
                            <li>H. Ma, T. Chen*, T. Hu*, C. You, X. Xie, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Undistillable: Making A Nasty Teacher That CANNOT Teach Students”</b><br> International Conference on Learning Representations (ICLR), 2021. (Spotlight Oral) <a href="https://openreview.net/forum?id=0zvfm-nZqQs">[Paper]</a><a href="https://github.com/VITA-Group/Nasty-Teacher">[Code]</a> 
                            <li>T. Chen*, Z. Zhang*, S. Liu, S. Chang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=LXMSvPmsm0g">[Paper]</a> <a href="https://github.com/VITA-Group/Lifelong-Learning-LTH">[Code]</a> 
                            <li>T. Chen*, Z. Zhang*, S. Liu, S. Chang, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Robust Overfitting May be Mitigated by Properly Learned Smoothening”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=qZzy5urZw9">[Paper]</a> <a href="https://github.com/VITA-Group/Alleviate-Robust-Overfitting">[Code]</a> 
                            <li>W. Chen*, X. Gong*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=Cnon5ezMHtu">[Paper]</a> <a href="https://github.com/VITA-Group/TENAS">[Code]</a> 
                            <li>W. Chen*, Z. Yu, S. Mello, S. Liu, J. Alvarez, Z. Wang, and A. Anandkumar<br> <b style="color:rgb(71, 71, 71)">“Contrastive Syn-to-Real Generalization”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=F8whUO8HNbP">[Paper]</a> <a href=" https://github.com/NVlabs/CSG">[Code]</a>      
                            <li>T. Meng, X. Chen*, Y. Jiang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“A Design Space Study for LISTA and Beyond”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=GMgHyUPrXa">[Paper]</a> <a href="https://github.com/google-research/google-research/tree/master/lista_design_space">[Code]</a> 
                            <li>J. Shen*, X. Chen*, H. Heaton, T. Chen*, J. Liu, W. Yin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Learning A Minimax Optimizer: A Pilot Study”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=nkIDwI6oO4_">[Paper]</a> <a href="https://github.com/VITA-Group/L2O-Minimax">[Code]</a> 
                            <li>J. Shen*, H. Wang*, S. Gui, J. Tan, Z. Wang, and J. Liu<br> <b style="color:rgb(71, 71, 71)">“UMEC: Unified Model and Embedding Compression for Efficient Recommendation Systems”</b><br> International Conference on Learning Representations (ICLR), 2021. <a href="https://openreview.net/forum?id=BM---bH_RSh">[Paper]</a> <a href="https://github.com/VITA-Group/UMEC">[Code]</a>
                            <li>J. Hong, H. Wang*, Z. Wang, and J. Zhou <br> <b style="color:rgb(71, 71, 71)">“Learning Model-Based Privacy Protection under Budget Constraints”</b><br> AAAI Conference on Artificial Intelligence (AAAI), 2021.<a href="https://www.aaai.org/AAAI21Papers/AAAI-7394.HongJ.pdf">[Paper]</a> [Code]
                            <li>T. Chen*, W. Zhang, J. Zhou, S. Chang, S. Liu, L. Amini, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Training Stronger Baselines for Learning to Optimize”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. (Spotlight Oral) <a href="https://arxiv.org/abs/2010.09089">[Paper]</a> <a href="https://github.com/VITA-Group/L2O-Training-Techniques">[Code]</a>
                            <li>H. Wang*, T. Chen*, S. Gui, T. Hu*, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://arxiv.org/abs/2010.11828">[Paper]</a> <a href="https://github.com/VITA-Group/Once-for-All-Adversarial-Training">[Code]</a>
                            <li>T. Chen*, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin<br> <b style="color:rgb(71, 71, 71)">“The Lottery Ticket Hypothesis for Pre-trained BERT Networks”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://arxiv.org/abs/2007.12223">[Paper]</a> <a href="https://github.com/VITA-Group/BERT-Tickets">[Code]</a>
                            <li>X. Chen*, Z. Wang, S. Tang, and K. Muandet<br> <b style="color:rgb(71, 71, 71)">“MATE: Plugging in Model Awareness to Task Embedding for Meta Learning”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://proceedings.neurips.cc/paper/2020/file/8989e07fc124e7a9bcbdebcc8ace2bc0-Paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/MATE">[Code]</a>
                            <li>Z. Jiang*, T. Chen*, T. Chen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Robust Pre-Training by Adversarial Contrastive Learning”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://arxiv.org/abs/2010.13337">[Paper]</a> <a href="https://github.com/VITA-Group/ACL_Neurips20">[Code]</a>
                            <li>Y. You*, T. Chen*, Y. Sui, T. Chen, Z. Wang, and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“Graph Contrastive Learning with Augmentations”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://arxiv.org/abs/2010.13902">[Paper]</a> <a href="https://github.com/VITA-Group/GraphCL">[Code]</a>
                            <li>H. You, X. Chen*, Y. Zhang, C. Li, S. Li, Z. Liu, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“ShiftAddNet: A Hardware-Inspired Deep Network”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://arxiv.org/abs/2010.12785">[Paper]</a> <a href="https://github.com/VITA-Group/ShiftAddNet">[Code]</a>
                            <li>Y. Fu, H. You, Y. Zhao, Y. Wang, C. Li, K. Gopalakrishnan, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“FracTrain: Fractionally Squeezing Bit Savings Both Temporally and Spatially for Efficient DNN Training”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2020. <a href="https://proceedings.neurips.cc/paper/2020/file/8dc5983b8c4ef1d8fcd5f325f9a65511-Paper.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/FracTrain">[Code]</a>
                            <li>Z. Wu*, D. Hoang*, S. Lin, Y. Xie, L. Chen, Y. Lin, Z. Wang, and W. Fan<br> <b style="color:rgb(71, 71, 71)">“MM-Hand: 3D-Aware Multi-Modal Guided Hand Generation for 3D Hand Pose Synthesis”</b><br> ACM International Conference on Multimedia (ACM MM), 2020. <a href="https://arxiv.org/abs/2010.01158">[Paper]</a> <a href="https://github.com/ScottHoang/mm-hand">[Code]</a></li>
                            <li>H. Wang*, S. Gui, H. Yang, J. Liu, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework”</b><br> European Conference on Computer Vision (ECCV), 2020. (Spotlight Oral) <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490052.pdf">[Paper]</a><a href="https://github.com/VITA-Group/GAN-Slimming">[Code]</a></li>
                            <li>S. Yang*, Z. Wang, J. Liu, and Z. Guo<br> <b style="color:rgb(71, 71, 71)">“Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches”</b><br> European Conference on Computer Vision (ECCV), 2020. <a href="https://arxiv.org/abs/2001.02890">[Paper]</a> <a href="https://github.com/VITA-Group/DeepPS">[Code]</a></li>
                            <li>C. Li, T. Chen*, H. You, Z. Wang, and Y Lin<br> <b style="color:rgb(71, 71, 71)">“HALO: Hardware-Aware Learning to Optimize”</b><br> European Conference on Computer Vision (ECCV), 2020. <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540477.pdf">[Paper]</a> <a href="https://github.com/RICE-EIC/HALO">[Code]</a></li>
                            <li>Z. Huo, A. PakBin, X. Chen*, N. Hurley, Y. Yuan*, X. Qian, Z. Wang, S. Huang, and B. Mortazavi<br> <b style="color:rgb(71, 71, 71)">“Uncertainty Quantification for Deep Context-Aware Mobile Activity Recognition and Unknown Context Discovery”</b><br>International Conference on Artificial Intelligence and Statistics (AISTATS), 2020. <a href="https://arxiv.org/abs/2003.01753">[Paper]</a> [Code]</li>
                            <li>W. Chen*, Z. Yu, Z. Wang, and A. Anandkumar<br> <b style="color:rgb(71, 71, 71)">“Automated Synthetic-to-Real Generalization”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="http://proceedings.mlr.press/v119/chen20x/chen20x.pdf">[Paper]</a> <a href="https://github.com/VITA-Group/ASG">[Code]</a></li>
                            <li>X. Chen*, W. Chen*, T. Chen*, Y. Yuan*, C. Gong, K. Chen, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Self-PU: Self Boosted and Calibrated Positive-Unlabeled Training”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="https://arxiv.org/abs/2006.11280">[Paper]</a> <a href="https://github.com/TAMU-VITA/Self-PU">[Code]</a></li>
                            <li>Y. You*, T. Chen*, Z. Wang, and Y. Shen <br> <b style="color:rgb(71, 71, 71)">“When Does Self-Supervision Help Graph Convolutional Networks?”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="https://arxiv.org/abs/2006.09136">[Paper]</a> <a href="https://github.com/Shen-Lab/SS-GCNs">[Code]</a></li>
                            <li>R. Oftadeh, J. Shen*, Z. Wang, and D. Shell<br> <b style="color:rgb(71, 71, 71)">“Eliminating the Invariance on the Loss Landscape of Linear Autoencoders”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="http://proceedings.mlr.press/v119/oftadeh20a/oftadeh20a.pdf">[Paper]</a> [Code]</li>
                            <li>Y. Fu, W. Chen*, H. Wang*, H. Li, Y. Lin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="https://arxiv.org/abs/2006.08198">[Paper]</a> <a href="https://github.com/TAMU-VITA/AGD">[Code]</a></li>
                            <li>R. Ardywibowo, S. Boluki, X. Gong*, Z. Wang, and X. Qian<br> <b style="color:rgb(71, 71, 71)">“NADS: Neural Architecture Distribution Search for Uncertainty Awareness”</b><br> International Conference on Machine Learning (ICML), 2020. <a href="https://arxiv.org/abs/2006.06646">[Paper]</a> <a href="https://github.com/ardywibowo/NADS">[Code]</a></li>
                            <li>Y. Zhao, X. Chen*, Y. Wang, C. Li, Y. Xie, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“SmartExchange: Trading Higher-cost Memory Storage/Access for Lower-cost Computation”</b><br> IEEE/ACM International Symposium on Computer Architecture (ISCA), 2020. <a href="https://arxiv.org/abs/2005.03403">[Paper]</a> [Code]</li>
                            <li>T. Chen*, S. Liu, S. Chang, Y. Cheng, L. Amini, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Adversarial Robustness: From Self-Supervised Pretraining to Fine-Tuning”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Adversarial_Robustness_From_Self-Supervised_Pre-Training_to_Fine-Tuning_CVPR_2020_paper.pdf">[Paper]</a> <a href="https://github.com/TAMU-VITA/Adv-SS-Pretraining">[Code]</a></li>
                            <li>Z. Jiang*, B. Liu, S. Schulter, Z. Wang, and M. Chandraker<br> <b style="color:rgb(71, 71, 71)">“Peek-a-boo: Occlusion Reasoning in Indoor Scenes with Plane Representations”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. (Oral) <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_Peek-a-Boo_Occlusion_Reasoning_in_Indoor_Scenes_With_Plane_Representations_CVPR_2020_paper.pdf">[Paper]</a> [Code]</li>
                            <li>Y. You*, T. Chen*, Z. Wang, and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“L<sup>2</sup>-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. <a href="https://arxiv.org/abs/2003.13606">[Paper]</a> <a href="https://github.com/TAMU-VITA/L2-GCN">[Code]</a></li>
                            <li>T. Hu*, T. Chen*, H. Wang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">"Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference"</b><br> International Conference on Learning Representations (ICLR), 2020. <a href="https://openreview.net/forum?id=rJgzzJHtDB">[Paper]</a> <a href="https://github.com/TAMU-VITA/triple-wins">[Code]</a></li>
                            <li>W. Chen*, X. Gong*, X. Liu, Q. Zhang, Y. Li and Z. Wang<br> <b style="color:rgb(71, 71, 71)">"FasterSeg: Searching for Faster Real-time Semantic Segmentation"</b><br> International Conference on Learning Representations (ICLR), 2020. <a href="https://openreview.net/forum?id=BJgqQ6NYvB">[Paper]</a> <a href="https://github.com/TAMU-VITA/FasterSeg">[Code]</a></li>
                            <li>H. Wang*, T. Chen*, Z. Wang, and K. Ma<br> <b style="color:rgb(71, 71, 71)">"I am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively"</b><br> International Conference on Learning Representations (ICLR), 2020. <a href="https://openreview.net/forum?id=rJehNT4YPr">[Paper]</a> <a href="https://github.com/TAMU-VITA/MAD">[Code]</a></li>
                            <li>H. You, C. Li, P. Xu, Y. Fu, Y. Wang, X. Chen*, R. Baraniuk, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks"</b><br> International Conference on Learning Representations (ICLR), 2020. (Spotlight Oral) <a href="https://openreview.net/forum?id=BJxsrgStvr">[Paper]</a> <a href="https://github.com/RICE-EIC/Early-Bird-Tickets">[Code]</a></li>
                            <li>J. Shen*, Y. Wang*, P. Xu, Y. Fu, Z. Wang, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“Fractional Skipping: Toward Finer-Grained Dynamic Inference”</b><br> AAAI Conference on Artificial Intelligence (AAAI), 2020. <a href="https://arxiv.org/abs/2001.00705">[Paper]</a> <a href="https://github.com/VITA-Group/DFS">[Code]</a></li>
                            <li>S. Mohseni*, M. Pitale, J. Yadawa, and Z. Wang<br> <b style="color:rgb(71, 71, 71)"> “Self-Supervised Learning for Generalizable Out-of-Distribution Detection”</b><br> AAAI Conference on Artificial Intelligence (AAAI), 2020. <a href="http://people.tamu.edu/~sina.mohseni/papers/Self_Supervised_Learning_for_Generalizable_Out_of_Distribution_Detection.pdf">[Paper]</a> [Code]</li>
                            <li>Z. Jiang*, Y. Wang*, X. Chen*, P. Xu, Y. Zhao, Y. Lin, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“E<sup>2</sup>-Train: Training State-of-the-art CNNs with Over 80% Energy Savings”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2019. <a href="https://arxiv.org/abs/1910.13349">[Paper]</a> <a href="https://github.com/RICE-EIC/E2Train">[Code]</a></li>
                            <li>S. Gui, H. Wang*, H. Yang, C. Yu, Z. Wang, and J. Liu<br> <b style="color:rgb(71, 71, 71)">“Model Compression with Adversarial Robustness: A Unified Optimization Framework”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2019. <a href="https://arxiv.org/abs/1902.03538">[Paper]</a> <a href="https://github.com/TAMU-VITA/ATMC">[Code]</a></li>
                            <li>Y. Cao, T. Chen*, Z. Wang, and Y. Shen<br> <b style="color:rgb(71, 71, 71)">“Learning to Optimize in Swarms”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2019. <a href="https://arxiv.org/abs/1911.03787">[Paper]</a> <a href="https://github.com/Shen-Lab/LOIS">[Code]</a></li>
                            <li>X. Jia, S. Wang*, X. Liang, A. Balagopal, D. Nguyen, M. Yang, Z. Wang, X. Qian, X. Ji, and S. Jiang<br> <b style="color:rgb(71, 71, 71)">“Cone-Beam Computed Tomography (CBCT) Segmentation by Adversarial Learning Domain Adaptation”</b><br> Medical Image Computing and Computer Assisted Interventions (MICCAI), 2019 <a href="https://link.springer.com/chapter/10.1007/978-3-030-32226-7_63">[Paper]</a> [Code]</li>
                              <li>R. Ardywibowo, G. Zhao, Z. Wang, B. Mortazavi, S. Huang, and X. Qian, <br> <b style="color:rgb(71, 71, 71)">“Activity Monitoring with Uncertainty Quantification in Switching Gaussian Process Models”</b><br> International Conference on Artificial Intelligence and Statistics (AISTATS), 2019 <a href="http://proceedings.mlr.press/v89/ardywibowo19a.html">[Paper]</a> [Code]</li>
                            <li>S. Yang*, Z. Wang, Z Wang, N. Xu, J. Liu, and Z. Guo<br> <b style="color:rgb(71, 71, 71)">“Controllable Artistic Text Style Transfer via Shape-Matching GAN”</b><br> IEEE International Conference on Computer Vision (ICCV), 2019. (Oral) <a href="https://arxiv.org/abs/1905.01354">[Paper]</a> <a href="https://github.com/TAMU-VITA/ShapeMatchingGAN">[Code]</a></li>
                            <li>Z. Wu*, K. Suresh, P. Narayanan, H. Xu, H. Kwon, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Delving into Robust Object Detection from Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach”</b><br> IEEE International Conference on Computer Vision (ICCV), 2019. <a href="https://arxiv.org/abs/1908.03856">[Paper]</a> <a href="https://github.com/TAMU-VITA/UAV-NDFT">[Code]</a></li>
                            <li>X. Gong*, S. Chang, Y. Jiang*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“AutoGAN: Neural Architecture Search for Generative Adversarial Networks”</b><br> IEEE International Conference on Computer Vision (ICCV), 2019. <a href="https://arxiv.org/abs/1908.03835">[Paper]</a> <a href="https://github.com/TAMU-VITA/AutoGAN">[Code]</a></li>
                            <li>T. Chen*, S. Ding, J. Xie, Y. Yuan*, W. Chen*, Y. Yang, Z. Ren, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“ABD-Net: Attentive but Diverse Person Re-Identification”</b><br> IEEE International Conference on Computer Vision (ICCV), 2019. <a href="https://arxiv.org/abs/1908.01114">[Paper]</a> <a href="https://github.com/TAMU-VITA/ABD-Net">[Code]</a></li>
                            <li>O. Kupyn, T. Martyniuk, J. Wu*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better”</b><br> IEEE International Conference on Computer Vision (ICCV), 2019. <a href="https://arxiv.org/abs/1908.03826">[Paper]</a> <a href="https://github.com/TAMU-VITA/DeblurGANv2">[Code]</a></li>
                            <li>E. Ryu, J. Liu, S. Wang*, X. Chen*, Z. Wang, and W. Yin<br> <b style="color:rgb(71, 71, 71)">“Plug-and-Play Methods Provably Converge with Properly Trained Denoisers”</b><br> International Conference on Machine Learning (ICML), 2019. <a href="https://arxiv.org/abs/1905.05406">[Paper]</a> <a href="https://github.com/TAMU-VITA/Provable_Plug_and_Play">[Code]</a></li>
                            <li>W. Chen*, Z. Jiang*, Z. Wang, K. Cui, and X. Qian<br> <b style="color:rgb(71, 71, 71)">“Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-high Resolution Images”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. (Oral) <a href="https://arxiv.org/abs/1905.06368">[Paper]</a> <a href="https://github.com/TAMU-VITA/GLNet">[Code]</a></li>
                            <li>S. Li, I. B. Araujo*, W. Ren, Z. Wang, E. K. Tokuda*, R. Hirata, R. Cesar, J. Zhang, X. Guo, and X. Cao<br> <b style="color:rgb(71, 71, 71)">“Single Image Deraining: A Comprehensive Benchmark Analysis”</b><br> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. <a href="https://arxiv.org/abs/1903.08558">[Paper]</a> <a href="https://github.com/lsy17096535/Single-Image-Deraining">[Code]</a></li>
                            <li>J. Liu, X. Chen*, Z. Wang, and W. Yin<br> <b style="color:rgb(71, 71, 71)">“ALISTA: Analytic Weights Are As Good As Learned Weights in LISTA”</b><br> International Conference on Learning Representations (ICLR), 2019. <a href="https://openreview.net/forum?id=B1lnzn0ctQ">[Paper]</a> <a href="https://github.com/TAMU-VITA/ALISTA">[Code]</a></li>
                            <li>X. Chen*, J. Liu, Z. Wang, and W. Yin<br> <b style="color:rgb(71, 71, 71)">“Theoretical Linear Convergence of Unfolded ISTA and Its Practical Weights and Thresholds”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2018. (Spotlight Oral) <a href="http://papers.nips.cc/paper/8120-theoretical-linear-convergence-of-unfolded-ista-and-its-practical-weights-and-thresholds">[Paper]</a> <a href="https://github.com/TAMU-VITA/LISTA-CPSS">[Code]</a></li>
                            <li>N. Bansal*, X. Chen*, and Z. Wang<br> <b style="color:rgb(71, 71, 71)">“Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?”</b><br> Advances in Neural Information Processing Systems (NeurIPS), 2018. <a href="http://papers.nips.cc/paper/7680-can-we-gain-more-from-orthogonality-regularizations-in-training-deep-networks">[Paper]</a> <a href="https://github.com/TAMU-VITA/Orthogonality-in-CNNs">[Code]</a></li>
                            <li>Z. Wu*, Z. Wang, Z. Wang, and H. Jin<br> <b style="color:rgb(71, 71, 71)">“Towards Privacy-Preserving Visual Recognition via Adversarial Training: A Pilot Study”</b><br> European Conference on Computer Vision (ECCV), 2018. <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Zhenyu_Wu_Towards_Privacy-Preserving_Visual_ECCV_2018_paper.html">[Paper]</a> <a href="https://github.com/TAMU-VITA/Privacy-AdversarialLearning">[Code]</a></li>
                            <li>M. Sun, I. Baytas, L. Zhan, Z. Wang, and J. Zhou<br> <b style="color:rgb(71, 71, 71)">“Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases”</b><br> ACM Conference on Knowledge Discovery and Data Mining (KDD), 2018. <a href="https://dl.acm.org/doi/abs/10.1145/3219819.3219966">[Paper]</a> <a href="https://github.com/illidanlab/subspace-net">[Code]</a></li>
                            <li>J. Wu*, Y. Wang*, Z. Wu*, Z. Wang, A. Veeraraghavan, and Y. Lin<br> <b style="color:rgb(71, 71, 71)">“Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions”</b><br> International Conference on Machine Learning (ICML), 2018. <a href="http://proceedings.mlr.press/v80/wu18h.html">[Paper]</a> <a href="https://github.com/TAMU-VITA/Deep-K-Means-pytorch">[Code]</a></li>
                        </ul>
                       
                    </div>
                </div>
            </div>

        </div>
        
<!-- END section -->


<div class="footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="copyright">
                    <p>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        Copyright &copy;<script>document.write(new Date().getFullYear());</script>
                        All rights reserved | Built upon <a
                            href="https://colorlib.com" target="_blank">Colorlib</a>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        </p>
                        
                </div>
            </div>
        </div>
    </div>
</div>

</div>
<!-- .site-wrap -->


<!-- loader -->
<!-- <div id="loader" class="show fullscreen">
    <svg class="circular" width="48px" height="48px">
        <circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/>
        <circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10"
                stroke="#ff5e15"/>
    </svg>
</div> -->

<script src="js/jquery-3.3.1.min.js"></script>
<script src="js/jquery-migrate-3.0.1.min.js"></script>
<script src="js/jquery-ui.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/owl.carousel.min.js"></script>
<script src="js/jquery.stellar.min.js"></script>
<script src="js/jquery.countdown.min.js"></script>
<script src="js/bootstrap-datepicker.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/aos.js"></script>
<script src="js/jquery.fancybox.min.js"></script>
<script src="js/jquery.sticky.js"></script>
<script src="js/jquery.mb.YTPlayer.min.js"></script>

<script src="js/main.js"></script>

</body>

</html>
